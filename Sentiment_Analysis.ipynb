{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/akalgreadis/deep-learning-playground/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KxVXIWjohky"
   },
   "source": [
    "# Deep learning Sentiment Analysis\n",
    "\n",
    "In this hands-on exercise we are going to learn to use Recurrent Neural Network (RNN) to perform sentiment analysis on movie reviews. \n",
    "\n",
    "The tools we are going to use for this exercise are :\n",
    "\n",
    "* **Tensorflow** - an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.\n",
    "\n",
    "\n",
    "* **Keras** - a heavyweight wrapper for Tensorflow. Itâ€™s minimalistic, modular, and awesome for rapid experimentation. This is our favorite Python library for deep learning and the best place to start for beginners.\n",
    "\n",
    "\n",
    "* **Google CoLab** - an implementation of the widely used open-source Jupyter Notebook that has most popular libraries used for machine learning already preinstalled. Its a perfect tool for rapid prototyping and experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "Wa-WVdcyohk_",
    "outputId": "70012beb-1549-43ac-8f53-304cbf234a50"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('https://preview.ibb.co/bC0Kfq/sentiment-Analysis-Workshop.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-2E5p2dohkz"
   },
   "source": [
    "# Step 1 - Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "edIyyG81ohk1",
    "outputId": "c00d6a6a-96a7-4082-c811-414c553ead4b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "import keras\n",
    "# Import the sequential model, which is a linear stack of layers\n",
    "from keras.models import Sequential \n",
    "# Import the core layers that are used in almost any neural netwrok\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Import the LSTM layer which is commonly used in RNN's\n",
    "from keras.layers import LSTM \n",
    "from keras.layers.embeddings import Embedding \n",
    "from keras.callbacks import TensorBoard\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2jAjEVFohk4"
   },
   "source": [
    "#Step 2 - Import the IMDb dataset \n",
    "\n",
    "As a first step, we will load the Internet Movie Database (IMDB) dataset, which is already part of the Keras datasets.\n",
    "\n",
    "The dataset consists of 25,000 movies reviews from IMDb, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Y7ADou88ohk6",
    "outputId": "8fcd5dec-a364-4067-fd01-4a45be423f74"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb \n",
    "# Define the n most common words we want to load\n",
    "top_words = 5000\n",
    "# Load the train and test datasets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jw_JsUzjpWp8"
   },
   "source": [
    "# Step 3 - Preprocess input data\n",
    "Not all reviews contain the same number of words, we will truncate the reviews in both the training and test datasets to a maximum number of words of 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "vzDlPC3FohlC",
    "outputId": "e60919b8-fb94-47c1-97a3-bb729433c033"
   },
   "outputs": [],
   "source": [
    "# Truncate and pad the review sequences\n",
    "from keras.preprocessing import sequence \n",
    "max_review_length = 500 \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length) \n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length) \n",
    "\n",
    "# Print the shape of the datasets, which should have a size of: \n",
    "# total number of reviews X number of words in the reviews\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6tNcy9t8nFB"
   },
   "source": [
    "##Print data sample\n",
    "\n",
    "If you run the cell below, we will print the first reviews.\n",
    "\n",
    "If you look at the data you will realize it has been already pre-processed. All words have been mapped to integers and the integers represent the words sorted by their frequency. It is very common in text analysis to represent a dataset like this. So 4 represents the 4th most used word, 5 the 5th most used word and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "5dNXowMWohlK",
    "outputId": "2b8272fb-6107-4833-e88f-fabaf8f14f45"
   },
   "outputs": [],
   "source": [
    "# Print the first reviews.\n",
    "pd.DataFrame(x_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnQ-VpT1mbKL"
   },
   "source": [
    "## Print movie review example \n",
    "\n",
    "Let's print an example of a movie review from the dataset.\n",
    "\n",
    "We only loaded the 5000 most common words from the dataset, therefore some words migh appear as < UNKNOWN >."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "mbmZd_C-msqi",
    "outputId": "241e6f28-a5a2-4094-ff1e-af03679ec786"
   },
   "outputs": [],
   "source": [
    "# Load the word indices such that we can convert the integers representations of the words in the dataset to strings\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNKOWN>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "# Define the review to print, try changing this number to see the print for different reviews\n",
    "review_number = 0\n",
    "# Print the review text\n",
    "print(' '.join(id_to_word[id] for id in x_train[review_number] ))\n",
    "# Print the sentiment of the review, 0 represents a negative review, 1 represents a positive review\n",
    "print('sentiment = ', y_train[review_number] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-vecLYPohlO"
   },
   "source": [
    "# Step 4 - Design model architecture\n",
    "\n",
    "The next step is to create the model that will be trained. We will use the Keras Sequential model, which is a linear stack of layers. For more information see https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "* The first layer is an embedding layer. https://keras.io/layers/embeddings/\n",
    "\n",
    "* The second layer is a Long Short Term Memory (LSTM) layer, which is a type of recurrent layer. \n",
    "\n",
    "* The third layer is a fully connected layer.\n",
    "\n",
    "The last step is to compile the model, for which we need to select both a loss function and an optimizer. Keras offer a wide selection and you can find more information here: \n",
    "\n",
    "* Loss functions : https://keras.io/losses/\n",
    "* Optimizers: https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "kgjTChkwohlP",
    "outputId": "f78fc383-b9ac-4b7d-bcb7-885a54fb753e"
   },
   "outputs": [],
   "source": [
    "embedding_vector_length = 32  #@param {type:\"slider\", min:16, max:64, step:16}\n",
    "number_recurrents = 2  #@param {type:\"slider\", min:2, max:100, step:2}\n",
    "\n",
    "# Create the model and add layers\n",
    "model = Sequential() \n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length)) \n",
    "model.add(LSTM(number_recurrents))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "# Compile the model with the selected loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdWFgGBfohlY"
   },
   "source": [
    "# Step 5 - Train the model\n",
    "\n",
    "Before we are ready to fit our model we need to select two more parameters, batch size and number of epochs.\n",
    "\n",
    "* **Batch size** : defines the number of samples that will be propagated through the network. Advantages of using a batch size smaller that your total samples is memory usage and efficiency in training.\n",
    "\n",
    "* **Epochs** : The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch.\n",
    "\n",
    "We will then fit the model which we defined in the previous step. This should take a little over 2 minutes per epoch. Notice that the training loss decreases and the accuracy increases over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "pbJJs61pohla",
    "outputId": "3d8ea7ad-ecd3-49c0-e555-701f0351cfc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Define model parameter values\n",
    "batch_size = 64 #@param {type:\"slider\", min:0, max:128, step:32}\n",
    "epochs = 3 #@param {type: \"slider\", min: 1, max: 8}\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=\"best_weights.hdf5\", monitor='val_loss', verbose=1, save_best_only=True)\n",
    "callbacks = [early_stopping, checkpoint]\n",
    "\n",
    "#convergence_history = model.fit(x_train[0:2500, :], y_train[0:2500], validation_data=(x_test[0:2500], y_test[0:2500]), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "convergence_history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "#convergence_history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=batch_size, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BpNKjEnGDm-"
   },
   "source": [
    "#Step 6 - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "4X2-ab2X0EZi",
    "outputId": "8274f01d-cf0e-4ce0-c071-fd9508be0c3f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(convergence_history.history['acc'])\n",
    "plt.plot(convergence_history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "B6hp9x4dMR4G",
    "outputId": "7c5d6700-8cc8-4830-ae8d-3ef2f03d6b12"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(convergence_history.history['loss'], label=\"Training loss\")\n",
    "plt.plot(convergence_history.history['val_loss'], label=\"Validation loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUlVlwpDz769"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxPmtZR9ohle"
   },
   "source": [
    "# Step 7 - Evaluate the model\n",
    "\n",
    "Use the test data to calculate the accuracy of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BHsubm8Xohlg",
    "outputId": "30dc2cec-fb4f-41c9-bf86-c715e50c0916"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-ytA_TWohlq"
   },
   "source": [
    "# Step 8 - Use the trained model\n",
    "Predict sentiment from the reviews:\n",
    "\n",
    "\"this movie was terrible and bad\"\n",
    "\"i really liked the movie and had fun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eHbsaAIPohlr",
    "outputId": "80675ad2-77c3-40dc-c14e-c34ae7a5e26a"
   },
   "outputs": [],
   "source": [
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(array([tmp_padded][0]))[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifF9Bp9QCqnQ"
   },
   "source": [
    "Now predict your own sentences with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2700
    },
    "colab_type": "code",
    "id": "StOHJEDKST_z",
    "outputId": "6fafc92e-e768-42d3-f8ef-a555bc10f53a"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "#@title Define your review and predict the sentiment\n",
    "badReview = \"\" #@param {type:\"string\"}\n",
    "goodReview = \"\" #@param {type:\"string\"}\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "for review in [goodReview,badReview]:\n",
    "    review = ''.join(ch for ch in review.lower() if ch not in exclude)\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(array([tmp_padded][0]))[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLG9eMieohlx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oHMvWi07jCR"
   },
   "source": [
    "# Step 9 - Complex (pre-trained) model\n",
    "\n",
    "Since the training of a RNN model takes some time. A pre trained model is created. The model, where the weights are tuned, will be loaded into the network. This will save the traning time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKV9GeHQ-v59"
   },
   "source": [
    "check dit voorbeeld laden https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-FS_1PcA2lr"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_weights.hdf5')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Sentiment_Analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
